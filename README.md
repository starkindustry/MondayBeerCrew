MondayBeerCrew
==============

It's Monday...where's the beer?

Testing: 
==============

For the analyzers, we currently use dummy dummy codebases and check the output manually. This includes number of classes exported, package names, dependencies, class size and complexity (as compared to other analyzing tools). An auxillary print method was added to the XMLParser class for this purpose. Basically the team would open the Fuser class and run the program on two or more XML input files and print the results to console. 

There are also some automated tests that were started for testing parsing of the results of DependencyFinder. But we aborted these as the requrements were changed late in the sprint which means that we will most likely have to change at least one of the analyzers. 
The current automated tests include: 

(unit test)
    Input: Mock objects (xml files generated by the analyzers on small dummy codebases) that return hard-coded values (e.g. stand-ins for your analysis tools).
    Output: Class name, package name, and number of dependencies
    Test: Compare against the expected value.

(unit test)
    Input: A dummy XML file with many elements.
    Output: A map of element->data.
    Test: Compare against a hard-coded map containing the correct values.

==============

For the visualizer, mock input csv files are used with limited amount of data and predictable visualizations, which were then visually inspected. 


(integration test, manual)
    Input: A small, dummy codebase for which you know roughly what the visualization should look like (like maybe even a single source file).
    Output: A visualization.
    Test: Look at the visualization and confirm that it looks/behaves as expected.